\documentclass{book}
\usepackage{fancyhdr, graphicx, multicol}

\title{\huge \textbf{The Hog Programming Language}}
\author{Jason Halpern \\ jrh2170 \\ Testing/Validation
        \and Samuel Messing \\ sbm2158 \\ Project Manager
        \and Benjamin Rapaport \\ bar2150 \\ System Architect
        \and Kurry Tran \\ klt2127 \\ System Integrator
        \and Paul Tylkin \\ pt2302 \\ Language Guru}

\begin{document}
\maketitle

\tableofcontents

\chapter{Introduction}
\label{chap:intro}

\section{Taming the Elephant}
\label{sec:elephant}

As data sets have grown in size, so have the complexities of dealing with them.
For instance, consider wanting to generate counts for all the words in
\emph{War and Peace} by means of distributed computation. Writing in Java and
using Hadoop MapReduce (TM),\footnote{\texttt{http://hadoop.apache.org/}} a
simple solution takes over 50 lines of code, as the programmer is required to
specify intermediate objects not directly related to the desired computation,
but required simply to get Hadoop to function properly. Our goal is to produce
a language that can express the same computation in about 10 lines.


Hog is a \textbf{data-oriented}, \textbf{high-level}, scripting language for
creating MapReduce\cite{dean:2004} programs. Used alongside Hadoop, Hog enables
users to efficiently carry out \textbf{distributed} computation. Hadoop
MapReduce is an open-source framework for carrying out distributed computation,
which is especially useful for working with large data sets. While it is
possible to write code to carry out computations with Hadoop directly, the
framework requires users to specify low-level details that are often irrelevant
to their desired goal.

By building a scripting language on top of Hadoop, we aim to simplify the
process. Built around a \textbf{simple} and highly \textbf{readable} syntax,
Hog will let users focus on \emph{what} computations they want done, and not
\emph{how} they want to do them.  Hog takes care of all the low-level details
required to run computations on Hadoop’s distributed network. All a user needs
to do is tell Hog the location of their valid Hadoop instance, and Hog will do
the rest. 

\subsection{Data-Oriented}
\label{sub:data-oriented}

Hog is a powerful language that allows for the efficient handling of
structured, unstructured and semi-structured data. Specifically, Hog simplifies
the process of writing programs to handle the distributed processing of
data-intensive applications. Programmers using Hog only have to express the
steps for processing the data in the Map and Reduce functions without having to
be concerned with relations and the constraints imposed by a traditional
database schema. Hog also provides control flow structures to manipulate this
data. In addition, Hog frees a programmer from having to write each step in a
data processing task since many of those low-level processing details are
handled by the language and the system.

Hog uses Hadoop MapReduce (TM), an open-source MapReduce framework written in
Java. Hadoop’s run time system takes care of the details of partitioning the
input data, scheduling the program’s execution across machines, counteracting
machine failures, and managing inter-machine communication. Hadoop also
distributes data to machines and tries to collocate chunks of data with the
nodes that need it, therefore maximizing data locality and giving good
performance.

\subsection{Simple}
\label{sub:simple}

To write a simple word count program in Java using the Hadoop framework
requires over 59 lines of
code.\footnote{\texttt{http://hadoop.apache.org/common/docs/current/mapred\_tutorial.html}}
The same program written in Hog requires just 10 lines. The discrepancy comes
from the fact that Hog takes care of the low-level details required to
correctly communicate and interact with the Hadoop framework. This allows users
to enhance the expressive potential of their programs, without sacrificing
power. All that Hog requires a user to do is specify the location of their
valid Hadoop instance, write a map function to process a segment of data, write
a reduce function to combine the results, and Hog takes care of the rest.

\chapter{Tutorial}
\label{chap:tutor}

Use your updated tutorial.

\chapter{Language Reference Manual}
\label{chap:LRM}

Use your update LRM.

\chapter{Project Plan}
\label{chap:plan}

\section{Development Process}

The scope of the Hog programming language was ambitious from the start. Our
stated goal was to create a general-purpose scripting language designed around
the Hadoop MapReduce framework. As such, from the beginning we were interested
in ways to make the implementation of the language as simple as possible.


\section{Roles and Responsibilities}

\begin{itemize}
\item Ben:
\item Jason:
\item Kurry:
\item Paul:
\item Sam:
\end{itemize}

\section{Hog's Developer Style Sheet}

To keep things as simple as possible.

\section{Project Timeline}

\section{Project Log}


\chapter{Language Evolution}
\label{chap:evo}

To be written by Paul.

\begin{itemize}
\item Describe how the language evolved during the implementation and what steps were used to try to maintain the good attributes of the original language proposal.
\item Describe the compiler tools used to create the compiler components.
\item Describe what unusual libraries are used in the compiler.
\item Describe what steps were taken to keep the LRM and the compiler consistent.
\end{itemize}

\chapter{Translator Architecture}
\label{chap:trans}

To be written by Ben.

\begin{itemize}
\item Show the architectural block diagram of translator.
\item Describe the interfaces between the modules.
\item State which modules were written by which team members.
\end{itemize}

\chapter{Development and Run-Time Environment}
\label{chap:environ}

To be written by Kurry.

\begin{itemize}
\item Describe the software development environment used to create the compiler.
\item Show the makefile used to create and test the compiler during development.
\item Describe the run-time environment for the compiler.
\end{itemize}

\chapter{Test Plan}
\label{chap:test}

To be written by Jason.

\begin{itemize}
\item Describe the test methodology used during development.
\item Show programs used to test your translator.
\end{itemize}

\chapter{Conclusions}
\label{chap:concl}

\section{Lessons Learned}
\label{sec:lessons}

\subsection{Jason's Lessons}
\label{sub:jasons-lessons}

To be written by Jason.

\subsection{Sam's Lessons}
\label{sub:sams-lessons}

To be written by Sam.

\subsection{Ben's Lessons}
\label{sub:bens-lessons}

To be written by Ben.

\subsection{Kurry's Lessons}
\label{sub:kurrys-lessons}

To be wrtten by Kurry.

\subsection{Paul's Lessons}
\label{sub:pauls-lessons}

To be written by Paul.

\section{Advice for Other Teams}

Don't take this class.

\section{Suggestions for Instructor}

Don't teach this class.

\appendix

\chapter{Code Listing}

Include a listing of the complete source code with identification of who wrote
which module of the compiler. This listing does not have to be included in the
paper copy of the final report.

\nocite{*}
\bibliographystyle{acm}
\bibliography{references}

\end{document}
