\documentclass{article} \usepackage{fancyhdr, multicol}

\title{\huge \textbf{Hog Language Tutorial}}

\begin{document}
\maketitle
\large
\begin{itemize}
  \item[] \textbf{Jason Halpern} $\hfill$ Testing/Validation
  \item[] \textbf{Samuel Messing} $\hfill$ Project Manager
  \item[] \textbf{Benjamin Rapaport} $\hfill$ System Architect
  \item[] \textbf{Kurry Tran} $\hfill$ System Integrator
  \item[] \textbf{Paul Tylkin} $\hfill$ Language Guru
\end{itemize}
\normalsize
\newpage

\section*{Introduction}
\label{sec:introduction}

Hog gives users with some programming experience a gentle introduction to
MapReduce, a popular programming model for distributed computation. In a Hog
program, a user specifies an \tt @Map \rm function, which operates on key-value
pairs (read from a text file), and outputs intermediate key-value pairs. The user
also specifies an \tt @Reduce \rm function, which groups the intermediate key-value
pairs by key, and outputs a final set of key-value pairs. This model of computation
has been widely adopted for distributing large computations that might be
considered "embarassingly parallelizable."

\subsection*{Program Structure} % (fold)
\label{sub:program_structure}
Every Hog program has four sections, defined in the following order:
\begin{description}
\item[\tt @Functions\rm:] An optional section which defines functions used throughout the program.
\item[\tt @Map\rm:] This section defines the map function that takes the input key-value pairs and outputs intermediate key-value pairs.
\item[\tt @Reduce\rm:] This section defines the reduce function that takes a single key from the set of intermediate key-value pairs output by the map function, and all of the values associated with that key, and reduces them to a final output.
\item [\tt @Main\rm:] The entry point for the program which initiates the MapReduce routine and can perform other local (non-distributed) computations.
\end{description}

\section*{Word Count} 
\label{word_count} 

Let's assume we have thousands of large text files, and we would like to get a
cross-file word count for each word that appears in any of the files. We also have
a cluster of computers to help us complete this task. The following short Hog
program will produce a single output file with each word and its associated count.

\subsection*{Word Count Code}
\begin{verbatim}
      
      @Map (int lineNum, text line)  ->  (text, int) {
            foreach word in line.tokenize(" ") {
                  emit(word, 1)
            }
      }
      
      @Reduce (text word, iter<int> values) -> (text, int) {
            int count = 0
            while (values.hasNext()) {
                  count = count + values.next()
            }
            emit(word, count)
      }
      
      @Main {
            mapReduce()
      }
      
\end{verbatim}

\subsection{Running the Word Count Code} % (fold)
\label{sub:running_the_word_count_code}

To run the program \tt WordCount.hog\rm, the user types the following into the
terminal: \\

\noindent \tt hog --hdfs WordCount.hog --input inputFile.txt
--output wordCounts.csv \rm \\

Here, \tt --hdfs \rm indicates that we want to run the job on the users Hadoop
cluster (specified in a configuration file, see the Hog's reference manual for
details on how to properly orient Hog to your Hadoop cluster). If, alternatively,
we wanted to run the job locally, we can type \tt --local\rm. The second parameter
is the name of the program, here \tt WordCount.hog\rm. The last two parameters
indicate the input file (or directory, in which case every file in the directory
will be used as input in turn) and the desired name of the output file.

% subsection running_the_word_count_code (end)

\subsection*{Word Count Explanation}

The general idea of this program is that we want to read every line of text from
every file, and then, grouping by word, output the total number of times we
encountered each word. Since we want to group by word, we will use the words
themselves as the intermediate key output by the \tt @Map \rm function. This will
allow us to group each word's value and send them all together in one key-value
pair to the \tt @Reduce \rm function.

\subsubsection*{\tt @Functions \rm}

The first thing we notice is that this program does not contain an \tt @Functions
\rm section. This section is optional, and only needs to be included if the user
wants to write his or her own subroutines to be used elsewhere in the program.

\subsubsection*{\tt @Map \rm}

This section's job is to read in a line of text from a file, and simply output each
word as the key with a value that indicates we have just encountered it. We will
use this value later to perform the summation.

The first line of this section is the \tt @Map \rm header, which defines the
\textbf{\emph{signature}} of the \tt @Map \rm function. In the current release, all
Hog programs read input files one line at a time, where the file offset of the line
is the key, and the text of the line is the value. \emph{This means that for all
Hog programs, the only allowable types for the input key-value pair is \tt (int\rm,
\tt text)\rm}. The inputs are also \textbf{\emph{named}} in the signature in order
to reference them in the body of the function.

The input signature is followed by an arrow, followed by the type signature of the
outputted intermediate key-value pairs. In this case, we will output each word as
\tt text \rm and its count as an \tt int\rm. These values are
\textbf{\emph{unnamed}}, as they cannot be referenced in the \tt @Map \rm section.

The \tt int \rm type represents an \textbf{\emph{integer number}} such as 0, 1, -2,
3, 5, etc. In addition, Hog has the type \tt real \rm which represents
\textbf{\emph{real numbers}} such as 0.1, 2.141, etc. The \tt text \rm type is
Hog's string type, and represents a sequence of characters. To create a \tt text
\rm object, simply include a string of characters between two double quotes (e.g.
\tt "hello world 123"\rm).

In the body of the function, we split the line of text passed in as the value into
words delineated by whitespace by using the built-in function \tt tokenize()\rm. We
then iterate through the \tt list \rm of words (of type \tt list<text>\rm) that \tt
tokenize() \rm returns using a \tt foreach \rm loop. Notice that you call \tt
tokenize() \rm "on" a \tt text \rm object. \tt text \rm objects are the only type
of objects that support this function. Attempting to call the function on an object
of a different type (e.g. \tt count.tokenize() \rm for the variable \tt count \rm
in this example) would lead to an error, called an \textbf{\emph{exception}}.
Exception handling is outside of the scope of this tutorial. Please see the
language reference manual for guidance on how to anticipate and handle exceptions.

In the body of the \tt foreach \rm loop, we use the built-in function \tt emit()
\rm to output a key-value pair, which the framework then groups by key when passing
to the \tt @reduce \rm section. In this case, since we want to group by the word
itself, we emit the word and the value \tt 1\rm, which we will later use to
calculate the totals in the \tt @Reduce \rm section.

\subsubsection*{\tt @Reduce \rm}


In this section, for each word (the key) emitted by the \tt @Map \rm section, we
will simply add up all the counts (the values) emitted for each particular word to
get the final count. It should now be clear why we emitted the valued \tt 1 \rm for
each word in the \tt @Map \rm section, as we do so once for every instance of
seeing a particular word.

Since the inputs to this section are grouped by key, \tt @Reduce \rm will receive a
word and an \textbf{\emph{iterator}} (referred to as an \tt iter \rm in Hog) over
all of that word's values (the \tt 1\rm's we emitted in the \tt @Map \rm section).
For \emph{every} word, this function will receive an iterator over all of the
values emitted by the \tt @Map \rm function for \emph{that} word. This is why the
header for this section has the word as the key and an iterator over a \tt list \rm
of \tt int\rm s as the value. The key type of the input to the reduce function
\emph{must match} the key type of the output of the map function. Similarly, the
values type of the reduce function is \emph{always} an iterator over the type of
the value output by the map function.

Since we want to output a word and its associated word count, \tt @Reduce \rm will
output \tt text \rm and \tt int \rm for each word.

In the body, we initialize the \textbf{\emph{variable}} \tt count \rm to \tt 0\rm,
and then iterate through the list of values using a familiar \tt while \rm loop,
adding each value of \tt 1 \rm to a running total (recall that \tt count \rm has
type \tt int\rm, which means it can represent an integer value). To do this, we use
the built-in functions on iterators \tt hasNext()\rm---which returns \tt true \rm
if the iterator contains more values and \tt false \rm otherwise---and \tt
next()\rm---which returns the next value in the \tt list \rm and moves the
iterator position forward. The statements inside the \tt while \rm loop continue to
execute until we have seen every variable in the \tt iter \rm object (when \tt
values.hasNext() \rm evalutes to \tt false\rm).

After we have a full count for the input word, we emit the word and its total count
as our final output. The framework then takes care of writing these emitted
key-value pairs to an output file, the name of which is specified by the user.

\subsubsection*{@Main}
In this section, we simply call the built-in function mapReduce(), which initiates the mapReduce program as specified by the previous sections and the command line arguments.


\section*{Merge Sort}
\label{merge_sort}
In this example, we will sort numbers in text files using a version of merge sort. We will assume that our text files contain lines of integers, delimited by commas. The idea is for each call to map to sort a small list of numbers on a single line of text, and for reduce to merge all of the sorted lists it receives.

\subsection*{Merge Sort Code}
\begin{verbatim}
      
      @Functions: {
	  
      # merge: Takes two sorted lists and merges them to return a new larger sorted list
      list<int> merge(list<int> sortedList1, list<int> sortedList2) {
      	  
            list<int> mergedList()
	
            # pointers to next value of each sorted list
            int ind1 = 0
            int ind2 = 0

            # merge all values while neither list is empty
            while( ind1 < sortedList1.size() && ind2 < sortedList2.size() ) {

                  # insert the smaller of the 2 values and update index pointers
                  if(sortedList1.get(ind1) < sortedList2.get(ind2)) {
                        mergedList.add(sortedList1.get(ind1))
                        ind1 = ind1 + 1
                  }
                  else { 
                        mergedList.add(sortedList2.get(ind2))
                        ind2 = ind2 + 2
                  }
            }
            
            # insert any remaining elements from sortedList1
            while (ind1 < sortedList1.size()) {
                  mergedList.add(sortedList1.get(ind1)
		          ind1 = ind1 + 1
		    }
		
		    # insert any remaining elements from sortedList2
            while (ind2 < sortedList2.size()) {
                  mergedList.add(sortedList2.get(ind2))
		          ind2 = ind2 + 1
		    }
	        
	        return mergedList
      }
      }
      
      @Map: (int lineNum, text line) -> (text, list<int>) {

            text reduceKey = "reduceKey"
            list<int> sortedInts()
            
            # put every number from line into list
            foreach number in line.tokenize(",") {
                  sortedInts.add((int) (number))
            }
            
            # sort list
            sortedInts.sort()
            
            # for every line of numbers, emit the sorted ints with an identical key
            emit(reduceKey, sortedInts)
      
      }
      
      # reduce will get a list of sorted lists, and merge them 2 at a time
      @Reduce: (text key, iter<list<int>> allSortedLists) -> (text, list<int>) {
            
            # only one output key
            text reduceKey = ""
            
            # begin with the first list as the fully sorted list 
            list<int> allSortedNums = allSortedLists.getNext()
            
            # merge the allSortedNums with the next sorted list until all lists have been merged
            while(allSortedLists.hasNext()) {
                  allSortedNums = merge(allSortedNums, allSortedLists.getNext())
            }
            
            emit(reduceKey, allSortedNums)
            
      }
      
      @Main: {
            print("Beginning sort.\n")
            mapReduce()
            print("Sort complete.\n")
      }

\end{verbatim}

\subsubsection*{@Functions}
In this section, we define a function called merge, which takes two sorted lists of ints, and returns a merged list of the two in sorted order. The way to define a function should be familiar to programmers comfortable with C or Java. In the first line of the body of the function, we are creating a new, empty list. Following that, we demonstrate a few flow of control statements such as while loops, and if else statements, the \&\& (and) boolean operator, and comparators all of which should also be familiar. 

Also included in this section are some built-in list functions, such as .size() to get the number of elements in a list, .add() to add an element onto the end of the list, and .get() to get an element at a specific index in the list.

\subsubsection*{@Map}
The map function reads in a line of comma separated integers as text, and outputs a list of the integers in sorted order. To do this, we introduce casting, which much always be explicit. In order to cast, the programmer must put the type he or she wants to cast to in parenthesis before the value or variable name. In this case, we are casting text to an int, which is a very common operation in Hog, since all input is read in as text.

To sort the list of ints that have been read in from the input, we call a built-in function on lists that contain primitives called .sort(). This function sorts the list in ascending order.

Finally, we emit the sorted list as a value, with identical keys for each list, so that they are all sent to a single reducer.

\subsection*{@Reduce}
The reduce function receives an iterator to all of the sorted lists from the map function, and merges them together one by one using the merge() function we defined earlier.

\subsection*{@Main}
In this section, we demonstrate that arbitrary code can be performed locally in the @Main block. While the @Main must always call the mapReduce() function to begin the map reduce program, it can also perform locally any code that could be written in a function. In this example, we use the built-in function print() to print to standard output and let the user know that the mapReduce job has completed.


\end{document}
