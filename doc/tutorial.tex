\documentclass{article} \usepackage{fancyhdr, multicol}

\title{\huge \textbf{Hog Language Tutorial}}

\begin{document}
\maketitle
\large
\begin{itemize}
  \item[] \textbf{Jason Halpern} $\hfill$ Testing/Validation
  \item[] \textbf{Samuel Messing} $\hfill$ Project Manager
  \item[] \textbf{Benjamin Rapaport} $\hfill$ System Architect
  \item[] \textbf{Kurry Tran} $\hfill$ System Integrator
  \item[] \textbf{Paul Tylkin} $\hfill$ Language Guru
\end{itemize}
\normalsize
\newpage

\section{Introduction}
\label{sec:introduction}

Hog gives users with some programming experience a gentle introduction to
MapReduce, a popular programming model for distributed computation. In a Hog
program, a user specifies an \tt @Map \rm function, which operates on key-value
pairs (read from a text file), and outputs intermediate key-value pairs. The user
also specifies an \tt @Reduce \rm function, which groups the intermediate key-value
pairs by key, and outputs a final set of key-value pairs. This model of computation
has been widely adopted for distributing large computations that might be
considered ``embarassingly parallelizable.''

\subsection{Thinking in MapReduce}
\label{sec:mapreduce_intro}
For a programmer new to the idea of distributed computation, MapReduce may appear to offer almost limitless possibilities. Part of the challenge of learning to use a MapReduce framework is to clearly understand its limitations. Intuitively, MapReduce can solve a problem only if there is an explicit parallelizable component. For example, finding the maximum value in a large unsorted list of numbers, computing a Fast Fourier Transform, and counting the occurrence of words in a large amount of text are problems with an easy-to-see parallelizable component. Here are the basic steps that can guide the programmer in identifying problems that are candidates for solution via MapReduce:
\begin{enumerate}
\item Construct a non-parallel solution to the problem.
\item Is there a component in the solution that can be broken up into smaller chunks that are not mutually dependent (the ``map'' phase)? If not, then MapReduce likely will not help.
\item Can the results of the computations of the independent smaller chunks be reassembled to produce a solution to a larger subproblem (the ``reduce'' phase)? If not, then MapReduce will not be able to build the solution to larger problems from smaller ones, and will not be useful.
\item Having identified the parallelizable component, check for hidden dependencies between the subproblems by analyzing whether reassembling the smaller components in different ways may produce different answers. If this happens, the reduce phase can not properly be completed.
\end{enumerate}

Once the map (breaking up the problem into chunks) and the reduce (recombining the solutions of the subproblems) phases have been clearly understood, one is ready to write a Hog program.

\subsection{Program Structure} % (fold)
\label{sub:program_structure}
Every Hog program has four sections, defined in the following order:
\begin{description}
\item[\tt @Functions\rm:] An optional section which defines functions used throughout the program.
\item[\tt @Map\rm:] This section defines the map function that takes the input key-value pairs and outputs intermediate key-value pairs.
\item[\tt @Reduce\rm:] This section defines the reduce function that takes a single key from the set of intermediate key-value pairs output by the map function, and all of the values associated with that key, and reduces them to a final output.
\item [\tt @Main\rm:] The entry point for the program which initiates the MapReduce routine and can perform other local (non-distributed) computations.
\end{description}

\section{Word Count} 
\label{word_count} 

Let's assume we have thousands of large text files, and we would like to get a
cross-file word count for each word that appears in any of the files. We also have
a cluster of computers to help us complete this task. The following short Hog
program will produce a single output file with each word and its associated count
on a separate line.

\subsection{Word Count Code}
\begin{verbatim}

  @Map (int lineNum, text line) -> (text, int) {
      # for every word on this line, emit that word and the number 1
      foreach text word in line.tokenize(" ") {
	  emit(word, 1);
      }
  }

  @Reduce (text word, iter<int> values) -> (text, int) {
      # initialize count to zero
      int count = 0;
      while( values.hasNext() ) {
	  # for every instance of '1' for this word, add to count
	  count = count + values.next();
      }
      # emit the count for this particular word
      emit(word, count);
   }

   @Main {
      # call map reduce
      mapReduce();
  }
      
\end{verbatim}

\subsection{Running the Word Count Code} % (fold)
\label{sub:running_the_word_count_code}

To run the program \tt WordCount.hog\rm, the user enters the input file (or input folder if there are several input files) and output folder in the Makefile and types the following into the
terminal: \\

\noindent \tt ./Hog.sh WordCount.hog  \rm \\

% subsection running_the_word_count_code (end)

\subsection{Word Count Explanation}

The general idea of this program is that we want to read every line of text from
every file, and then, grouping by word, output the total number of times we
encountered each word. Since we want to group by word, we will use the words
themselves as the intermediate key output by the \tt @Map \rm function. This will
allow us to group each word with its values and to send the key, value pairs with that word as the key to the \tt @Reduce \rm function.

\subsubsection{\tt @Functions \rm}

The first thing we notice is that this program does not contain an \tt @Functions
\rm section. This section is optional, and only needs to be included if the user
wants to write his or her own subroutines to be used elsewhere in the program.

\subsubsection{\tt @Map \rm}

This section's job is to read in a line of text from a file, and simply output each
word as the key with a value that indicates we have just encountered it. We will
use this value later to perform the summation.

The first line of this section is the \tt @Map \rm header, which defines the
\textbf{\emph{signature}} of the \tt @Map \rm function. \emph{For all
Hog programs, the only allowable types for the input key-value pair is \tt (int\rm,
\tt text)\rm}. The inputs are also \textbf{\emph{named}} in the signature in order
to reference them in the body of the function.

The input signature is followed by an arrow, followed by the type signature of the
outputted intermediate key-value pairs. In this case, we will output each word as
\tt text \rm and its count as an \tt int\rm. These values are
\textbf{\emph{unnamed}}, as they cannot be referenced in the \tt @Map \rm section.

The \tt int \rm type represents an \textbf{\emph{integer number}} such as 0, 1, -2,
3, 5, etc. In addition, Hog has the type \tt real \rm which represents
\textbf{\emph{real numbers}} such as 0.1, 2.141, etc. The \tt text \rm type is
Hog's string type, and represents a sequence of characters. To create a \tt text
\rm object, simply include a string of characters between two double quotes (e.g.
\tt "hello world 123"\rm).

In the body of the function, we split the line of text passed in using whitespace as the delimiter 
in the built-in function  \tt tokenize()\rm. We
then iterate through the \tt list \rm of words (of type \tt list<text>\rm) that \tt
tokenize() \rm returns using a \tt foreach \rm loop. Notice that you call \tt
tokenize() \rm on a \tt list \rm object.

In the body of the \tt foreach \rm loop, we use the built-in function \tt emit()
\rm to output a key-value pair, which the framework then groups by key when passing
to the \tt @Reduce \rm section. In this case, since we want to group by the word
itself, we emit the word and the value \tt 1\rm, which we will later use to
calculate the totals in the \tt @Reduce \rm section.

\subsubsection{\tt @Reduce \rm}


In this section, for each word (the key) emitted by the \tt @Map \rm section, we
will simply add up all the counts (the values) emitted for each particular word to
get the final count. It should now be clear why we emitted the value \tt 1 \rm for
each word in the \tt @Map \rm section, as we do so once for every instance of
seeing a particular word.

Since the inputs to this section are grouped by key, \tt @Reduce \rm will receive a
word and an \textbf{\emph{iterator}} (referred to as an \tt iter \rm in Hog) over
all of that word's values (the \tt 1\rm's we emitted in the \tt @Map \rm section).
For \emph{every} word, this function will receive an iterator over all of the
values emitted by the \tt @Map \rm function for \emph{that} word. This is why the
header for this section has the word as the key and an iterator over a \tt list \rm
of \tt int\rm s as the value. The key type of the input to the reduce function
\emph{must match} the key type of the output of the map function. Similarly, the
values type of the reduce function is \emph{always} an iterator over the type of
the value output by the map function.

Since we want to output a word and its associated count, \tt @Reduce \rm will
output \tt text \rm and \tt int \rm for each word.

In the body, we initialize the \textbf{\emph{variable}} \tt count \rm to \tt 0\rm,
and then iterate through the list of values associated with that word using a familiar \tt while \rm loop,
adding each value to a running total (recall that \tt count \rm has
type \tt int\rm, which means it can represent an integer value). To do this, we use
the built-in functions on iterators \tt hasNext()\rm---which returns \tt true \rm
if the iterator contains more values and \tt false \rm otherwise---and \tt
next()\rm---which returns the next value in the \tt list \rm and moves the
iterator position forward. The statements inside the \tt while \rm loop continue to
execute until we have seen every variable in the \tt iter \rm object (when \tt
values.hasNext() \rm evaluates to \tt false\rm).

After we have a full count for the input word, we emit the word and the total count
as our final output. The framework then takes care of writing these emitted
key-value pairs to an output file, the name of which is specified by the user when
the program is run (see the section above, \textbf{Running the Word Count Code}). In this example, we are emitting the word count as the key so that the final result is sorted by the count. If the word was used as the key, the final output would be in alphabetical order. It is also worth noting that even if two words have the same count (and thus the same key), this does not raise any problems, since duplicate keys are only combined in between the \tt @Map \rm and \tt @Reduce \rm phases. Any individual key-value pair emitted in the \tt @Reduce \rm phase will be in the final output.

\subsubsection{@Main}

In this section, we simply call the built-in function \tt mapReduce()\rm, which
initiates the MapReduce program as specified by the previous sections and the
command line arguments.

\subsubsection{Sample Output}

Below are the first fifty lines of output generated by \tt WordCount.hog \rm when run
on a single file containing the English text of \emph{War and Peace}:

\begin{verbatim}
  the, 31784
  and, 21049
  to, 16389
  a, 10056
  in, 8314
  he, 7847
  his, 7645
  that, 7425
  was, 7255
  with, 5540
  had, 5316
  not, 4492
  at, 4209
  her, 4162
  I, 4009
  it, 3757
  on, 3495
  hi, 348
  ....
\end{verbatim}

\subsection{Comparison with Java Hadoop Program}

One of the main goals of Hog is to abstract away the lower-level details of Hadoop. How useful this is to a programmer just learning how to MapReduce will become readily apparent when one considers an equivalent Hog and a Java/Hadoop program side-by-side.

Our Hog program is below:
\begin{verbatim}
 @Map (int lineNum, text line) -> (text, int) {
      foreach text word in line.tokenize(" ") {
	  emit(word, 1);
      }
  }

  @Reduce (text word, iter<int> values) -> (text, int) {
      int count = 0;
      while( values.hasNext() ) {
	  count = count + values.next();
      }
      emit(word, count);
   }

   @Main {
      mapReduce();
  }
\end{verbatim}      

A Java program carrying out exactly the same task is below (this actually comes from our compiler's Code Generator, and is very similar to the sample WordCount program on Hadoop's website, http://wiki.apache.org/hadoop/WordCount):

\begin{verbatim}
import java.io.IOException;
import java.util.*;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.util.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
public class Hog {
    public static class Map extends MapReduceBase implements 
     Mapper<LongWritable, Text, Text, IntWritable> {
        public void map(LongWritable lineNum, Text value,  
               OutputCollector<Text, IntWritable> output, 
               Reporter reporter) throws IOException {
            String line = value.toString();
            for (String word : line.split(" ")) {
                output.collect(new Text(word), new IntWritable(1));
             }
        }
    }
    public static class Reduce extends MapReduceBase 
              implements Reducer<Text, IntWritable, Text, IntWritable> {
        public void reduce(Text word, Iterator<IntWritable> values,  
               OutputCollector<Text, IntWritable> output, 
               Reporter reporter) throws IOException {
            int count = 0;
            while (values.hasNext()) {
                count = count + values.next().get();
             }
            output.collect(new Text(word), new IntWritable(count));
        }
    }
    public static void main(String[] args) throws Exception {
        JobConf conf = new JobConf(Hog.class);
        conf.setJobName("hog");
        conf.setOutputKeyClass(Text.class);
        conf.setOutputValueClass(IntWritable.class);
        conf.setMapperClass(Map.class);
        conf.setCombinerClass(Reduce.class);
        conf.setReducerClass(Reduce.class);
        conf.setInputFormat(TextInputFormat.class);
        conf.setOutputFormat(TextOutputFormat.class);
        FileInputFormat.setInputPaths(conf, new Path(args[0]));
        FileOutputFormat.setOutputPath(conf, new Path(args[1]));
        JobClient.runJob(conf);
    }
}
\end{verbatim}

In addition to the length, the Java program contains many constructs that are intimidating to the novice MapReducer, and requires a substantial learning curve to understand and remember (missing even one of the statements in the above program or putting them in the wrong order may make the program not executable). In stark constrast to this, we expect that users will be able to quickly learn and use Hog to write programs, enjoying its substantially more intuitive grammar and structure.

\section{MergeSort and User-Defined Functions} \label{merge_sort} 

In this example, we will sort numbers in text files using a distributed version of
the algorithm MergeSort. We will assume that our text files
contain lines of integers, delimited by spaces. The idea is for each call to map to
sort a small list of numbers on a single line of text, and for reduce to merge all
of the sorted lists it receives. The amazing thing about the Hadoop framework (paired with the simplicity and power of Hog), is how simple this program is. Although not necessary for sorting, we also introduce a user-defined function, \tt factorial\rm, and show how Hog can be used to perform simple non-MapReduce tasks.

\subsection{MergeSort Code}
\begin{verbatim}
  @Functions {
      int factorial (int n){
	  if (n == 0){
             return 1;
          } else{
             return n*factorial(n-1); 
          }
      }
  }
  @Map (int lineNum, text line) -> (text, text) {
      foreach text number in line.tokenize(" ") {
	  emit(number, number);
      }
   }

  @Reduce (text number, text something) -> (text, text) {
      emit(number, "");
  }
    
  @Main {
    print(factorial(10));
    mapReduce();
  }

\end{verbatim}

\subsubsection{\tt @Functions \rm}

This is not a necessary part of the MergeSort program, but has been added to demonstrate how the programmer may add user-defined function.
In this section, we define a function called \tt factorial\rm, which takes a (positive) \tt int\rm, 
and returns an \tt int \rm representing the factorial of that value. The way to define a function should be familiar to programmers
comfortable with C or Java. In addition, this example shows that Hog allows for recursive function definition.

\subsubsection{\tt @Map \rm} 

The map function reads in a line of space-delimited integers as \tt text\rm, and
outputs ordered pairs of the integer paired with itself (required because \tt emit\rm must have two arguments). This is a common trick employed in Hog
when we only care about one value instead of two (i.e. a key-value pair).

\subsubsection{\tt @Reduce \rm}

The reduce function receives an iterator over \emph{all} of the key-value pairs from
the map function, and merges them together, and emits them in sorted order. This nice side effect of the Hadoop reducer allows MergeSort to be almost completely trivial to write and execute in Hog.

\subsubsection{\tt @Main \rm}

In this section, we demonstrate that arbitrary code can be executed locally (i.e.
not on the cluster) in the \tt @Main \rm block. While the \tt @Main \rm must always
call the \tt mapReduce() \rm function to begin the map reduce program, it can also
perform locally any code that could be written in a function. In this example, we
use the built-in function \tt print() \rm to compute and print \tt factorial(10).\rm

\subsubsection{Sample Input and Output}

Sample input file:

\begin{verbatim}
37 96 14 
80 85 45 
58 69 27 
49
\end{verbatim}

Because we used the same, empty \tt text \rm ( \tt text reduceKey = "" \rm) for the
keys for all key-value pairs sent to the \tt @Reduce \rm section, we end up with a
sorted list of numbers all on one line. An example is given below,

\begin{verbatim}
 14 
 27 
 37 
 45 
 49 
 58 
 69 
 80 
 85 
 96
\end{verbatim}

And the console will contain the printed value ``3628800'', corresponding to 10!.

\end{document}
