\documentclass{article} \usepackage{fancyhdr, multicol}

\title{\huge \textbf{Hog Language Tutorial}}

\begin{document}
\maketitle
\large
\begin{itemize}
  \item[] \textbf{Jason Halpern} $\hfill$ Testing/Validation
  \item[] \textbf{Samuel Messing} $\hfill$ Project Manager
  \item[] \textbf{Benjamin Rapaport} $\hfill$ System Architect
  \item[] \textbf{Kurry Tran} $\hfill$ System Integrator
  \item[] \textbf{Paul Tylkin} $\hfill$ Language Guru
\end{itemize}
\normalsize
\newpage

\section*{Introduction}
\label{sec:introduction}

Hog gives users with some programming experience a gentle introduction to
MapReduce, a popular programming model for distributed computation. In a Hog
program, a user specifies an \tt @Map \rm function, which operates on key-value
pairs (read from a text file), and outputs intermediate key-value pairs. The user
also specifies an \tt @Reduce \rm function, which groups the intermediate key-value
pairs by key, and outputs a final set of key-value pairs. This model of computation
has been widely adopted for distributing large computations that might be
considered "embarassingly parallelizable."

\subsection*{Program Structure} % (fold)
\label{sub:program_structure}
Every Hog program has four sections, defined in the following order:
\begin{description}
\item[\tt @Functions\rm:] An optional section which defines functions used throughout the program.
\item[\tt @Map\rm:] This section defines the map function that takes the input key-value pairs and outputs intermediate key-value pairs.
\item[\tt @Reduce\rm:] This section defines the reduce function that takes a single key from the set of intermediate key-value pairs output by the map function, and all of the values associated with that key, and reduces them to a final output.
\item [\tt @Main\rm:] The entry point for the program which initiates the MapReduce routine and can perform other local (non-distributed) computations.
\end{description}

\section*{Word Count} 
\label{word_count} 

Let's assume we have thousands of large text files, and we would like to get a
cross-file word count for each word that appears in any of the files. We also have
a cluster of computers to help us complete this task. The following short Hog
program will produce a single output file with each word and its associated count.

\subsection*{Word Count Code}
\begin{verbatim}
      
      @Map (int lineNum, text line)  ->  (text, int) {
            foreach word in line.tokenize(" ") {
                  emit(word, 1)
            }
      }
      
      @Reduce (text word, iter<int> values) -> (text, int) {
            int count = 0
            while (values.hasNext()) {
                  count = count + values.next()
            }
            emit(word, count)
      }
      
      @Main {
            mapReduce()
      }
      
\end{verbatim}

\subsection*{Word Count Explanation}

The general idea of this program is that we want to read every line of text from
every file, and then, grouping by word, output the total number of times we
encountered each word. Since we want to group by word, we will use the words
themselves as the intermediate key output by the \tt @Map \rm function. This will
allow us to group each word's value and send them all together in one key-value
pair to the \tt @Reduce \rm function.

\subsubsection*{\tt @Functions \rm}

The first thing we notice is that this program does not contain an \tt @Functions
\rm section. This section is optional, and only needs to be included if the user
wants to write his or her own subroutines to be used elsewhere in the program.

\subsubsection*{\tt @Map \rm}

This section's job is to read in a line of text from a file, and simply output each
word as the key with a value that indicates we have just encountered it. We will
use this value later to perform the summation.

The first line of this section is the \tt @Map \rm header, which defines the
\textbf{\emph{signature}} of the \tt @Map \rm function. In the current release, all
Hog programs read input files one line at a time, where the file offset of the line
is the key, and the text of the line is the value. \emph{This means that for all
Hog programs, the only allowable types for the input key-value pair is \tt (int\rm,
\tt text)\rm}. The inputs are also \textbf{\emph{named}} in the signature in order
to reference them in the body of the function.

The input signature is followed by an arrow, followed by the type signature of the
outputted intermediate key-value pairs. In this case, we will output each word as
\tt text \rm and its count as an \tt int\rm. These values are
\textbf{\emph{unnamed}}, as they cannot be referenced in the \tt @Map \rm section.

The \tt int \rm type represents an \textbf{\emph{integer number}} such as 0, 1, -2,
3, 5, etc. In addition, Hog has the type \tt real \rm which represents
\textbf{\emph{real numbers}} such as 0.1, 2.141, etc. The \tt text \rm type is
Hog's string type, and represents a sequence of characters. To create a \tt text
\rm object, simply include a string of characters between two double quotes (e.g.
\tt "hello world 123"\rm).

In the body of the function, we split the line of text passed in as the value into
words delineated by whitespace by using the built-in function \tt tokenize()\rm. We
then iterate through the \tt list \rm of words (of type \tt list<text>\rm) that \tt
tokenize() \rm returns using a \tt foreach \rm loop.

In the body of the \tt foreach \rm loop, we use the built-in function \tt emit()
\rm to output a key-value pair, which the framework then groups by key. In this
case, since we want to group by the word itself, and so we emit the word and the
value \tt 1\rm, which we will use to calculate the totals in the \tt @Reduce \rm
section.

\subsubsection*{@Reduce}
In this section, for each word (the key) emitted by the @Map section, we will simply add up all the ones (the value) emitted as well to get a final count for each word. 

Since the inputs to this section are grouped by key, @Reduce will receive a word and an iterator over all of that word's values (the ones). For every word, this function will receive an iterator over all of the values emitted by the @Map function for that word. This is why the header for this section has the word as the key and an iterator over a list of ints as the value. The key type of the input to the reduce function must match the key type of the output of the map function. Similarly, the values type of the reduce function is simply an iterator over the type of the value output by the map function.

Since we want to output a word and its associated word count, @Reduce will output text and an int for each word.

In the body, we initialize the count to 0, and then iterate through the list of values using a familiar while loop, adding each value of one to a running total. To do this, we use the built-in functions on iterators hasNext(), which returns true or false, and getNext(), which returns the next value in the list and moves the iterator forward. 

After we have a full count for the input word, we emit the word and its count as our final output.

\subsubsection*{@Main}
In this section, we simply call the built-in function mapReduce(), which initiates the mapReduce program as specified by the previous sections and the command line arguments.


\section*{Merge Sort}
\label{merge_sort}
In this example, we will sort numbers in text files using a version of merge sort. We will assume that our text files contain lines of integers, delimited by commas. The idea is for each call to map to sort a small list of numbers on a single line of text, and for reduce to merge all of the sorted lists it receives.

\subsection*{Merge Sort Code}
\begin{verbatim}
      
      @Functions: {
	  
      # merge: Takes two sorted lists and merges them to return a new larger sorted list
      list<int> merge(list<int> sortedList1, list<int> sortedList2) {
      	  
            list<int> mergedList()
	
            # pointers to next value of each sorted list
            int ind1 = 0
            int ind2 = 0

            # merge all values while neither list is empty
            while( ind1 < sortedList1.size() && ind2 < sortedList2.size() ) {

                  # insert the smaller of the 2 values and update index pointers
                  if(sortedList1.get(ind1) < sortedList2.get(ind2)) {
                        mergedList.add(sortedList1.get(ind1))
                        ind1 = ind1 + 1
                  }
                  else { 
                        mergedList.add(sortedList2.get(ind2))
                        ind2 = ind2 + 2
                  }
            }
            
            # insert any remaining elements from sortedList1
            while (ind1 < sortedList1.size()) {
                  mergedList.add(sortedList1.get(ind1)
		          ind1 = ind1 + 1
		    }
		
		    # insert any remaining elements from sortedList2
            while (ind2 < sortedList2.size()) {
                  mergedList.add(sortedList2.get(ind2))
		          ind2 = ind2 + 1
		    }
	        
	        return mergedList
      }
      }
      
      @Map: (int lineNum, text line) -> (text, list<int>) {

            text reduceKey = "reduceKey"
            list<int> sortedInts()
            
            # put every number from line into list
            foreach number in line.tokenize(",") {
                  sortedInts.add((int) (number))
            }
            
            # sort list
            sortedInts.sort()
            
            # for every line of numbers, emit the sorted ints with an identical key
            emit(reduceKey, sortedInts)
      
      }
      
      # reduce will get a list of sorted lists, and merge them 2 at a time
      @Reduce: (text key, iter<list<int>> allSortedLists) -> (text, list<int>) {
            
            # only one output key
            text reduceKey = ""
            
            # begin with the first list as the fully sorted list 
            list<int> allSortedNums = allSortedLists.getNext()
            
            # merge the allSortedNums with the next sorted list until all lists have been merged
            while(allSortedLists.hasNext()) {
                  allSortedNums = merge(allSortedNums, allSortedLists.getNext())
            }
            
            emit(reduceKey, allSortedNums)
            
      }
      
      @Main: {
            print("Beginning sort.\n")
            mapReduce()
            print("Sort complete.\n")
      }

\end{verbatim}

\subsubsection*{@Functions}
In this section, we define a function called merge, which takes two sorted lists of ints, and returns a merged list of the two in sorted order. The way to define a function should be familiar to programmers comfortable with C or Java. In the first line of the body of the function, we are creating a new, empty list. Following that, we demonstrate a few flow of control statements such as while loops, and if else statements, the \&\& (and) boolean operator, and comparators all of which should also be familiar. 

Also included in this section are some built-in list functions, such as .size() to get the number of elements in a list, .add() to add an element onto the end of the list, and .get() to get an element at a specific index in the list.

\subsubsection*{@Map}
The map function reads in a line of comma separated integers as text, and outputs a list of the integers in sorted order. To do this, we introduce casting, which much always be explicit. In order to cast, the programmer must put the type he or she wants to cast to in parenthesis before the value or variable name. In this case, we are casting text to an int, which is a very common operation in Hog, since all input is read in as text.

To sort the list of ints that have been read in from the input, we call a built-in function on lists that contain primitives called .sort(). This function sorts the list in ascending order.

Finally, we emit the sorted list as a value, with identical keys for each list, so that they are all sent to a single reducer.

\subsection*{@Reduce}
The reduce function receives an iterator to all of the sorted lists from the map function, and merges them together one by one using the merge() function we defined earlier.

\subsection*{@Main}
In this section, we demonstrate that arbitrary code can be performed locally in the @Main block. While the @Main must always call the mapReduce() function to begin the map reduce program, it can also perform locally any code that could be written in a function. In this example, we use the built-in function print() to print to standard output and let the user know that the mapReduce job has completed.


\end{document}
